"""
LISAæ¨¡å‹è¯„ä¼°ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨eval.pyå¯¹LISAæ¨¡å‹è¿›è¡ŒRefCOCOå’ŒReasonSegæ•°æ®é›†è¯„ä¼°
"""
import os
import sys
import cv2
import numpy as np
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, CLIPImageProcessor
from PIL import Image

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥eval
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from eval import RefCOCODataset, ReasonSegDataset, predict_model_by_single_sample, compute_metrics

# å¯¼å…¥LISAæ¨¡å‹ç›¸å…³
from model.LISA import LISAForCausalLM
from model.llava import conversation as conversation_lib
from model.llava.mm_utils import tokenizer_image_token
from model.segment_anything.utils.transforms import ResizeLongestSide
from utils.utils import (DEFAULT_IM_END_TOKEN, DEFAULT_IM_START_TOKEN,
                         DEFAULT_IMAGE_TOKEN, IMAGE_TOKEN_INDEX)

# è®¾ç½®æ¨¡å‹ä¸‹è½½ç¼“å­˜ç›®å½•
CACHE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "model_download")
os.makedirs(CACHE_DIR, exist_ok=True)

# è®¾ç½®æœ¬åœ° SAM æƒé‡è·¯å¾„
SAM_CHECKPOINT_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "sam", "sam_vit_h_4b8939.pth")

print(f"æ¨¡å‹ç¼“å­˜ç›®å½•: {CACHE_DIR}")
print(f"SAM æƒé‡è·¯å¾„: {SAM_CHECKPOINT_PATH}")


def preprocess(
    x,
    pixel_mean=torch.Tensor([123.675, 116.28, 103.53]).view(-1, 1, 1),
    pixel_std=torch.Tensor([58.395, 57.12, 57.375]).view(-1, 1, 1),
    img_size=1024,
) -> torch.Tensor:
    """Normalize pixel values and pad to a square input."""
    # Normalize colors
    x = (x - pixel_mean) / pixel_std
    # Pad
    h, w = x.shape[-2:]
    padh = img_size - h
    padw = img_size - w
    x = F.pad(x, (0, padw, 0, padh))
    return x


# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹ç»„ä»¶
GLOBAL_MODEL = None
GLOBAL_TOKENIZER = None
GLOBAL_CLIP_PROCESSOR = None
GLOBAL_TRANSFORM = None
GLOBAL_ARGS = None


def load_model(
    version="xinlai/LISA-13B-llama2-v1",
    precision="fp32",
    vision_tower="openai/clip-vit-large-patch14",
    sam_checkpoint=None,
    image_size=1024,
    model_max_length=512,
    use_mm_start_end=True,
    conv_type="llava_v1",
    local_rank=0,
    **kwargs
):
    """
    åŠ è½½LISAæ¨¡å‹
    
    Args:
        version: æ¨¡å‹ç‰ˆæœ¬æˆ–è·¯å¾„
        precision: ç²¾åº¦ (fp32, fp16, bf16)
        vision_tower: CLIP vision tower
        sam_checkpoint: SAMæ¨¡å‹æƒé‡è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨ LISA/sam/sam_vit_h_4b8939.pth
        image_size: å›¾åƒå°ºå¯¸
        model_max_length: æœ€å¤§åºåˆ—é•¿åº¦
        use_mm_start_end: æ˜¯å¦ä½¿ç”¨å›¾åƒèµ·æ­¢æ ‡è®°
        conv_type: å¯¹è¯æ¨¡æ¿ç±»å‹
        local_rank: è®¾å¤‡rank
    """
    global GLOBAL_MODEL, GLOBAL_TOKENIZER, GLOBAL_CLIP_PROCESSOR, GLOBAL_TRANSFORM, GLOBAL_ARGS
    
    # ä½¿ç”¨é»˜è®¤çš„æœ¬åœ° SAM æƒé‡è·¯å¾„
    if sam_checkpoint is None:
        sam_checkpoint = SAM_CHECKPOINT_PATH
    
    # æ£€æŸ¥ SAM æƒé‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(sam_checkpoint):
        print(f"âš ï¸  è­¦å‘Š: SAM æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {sam_checkpoint}")
        print("   è¯·ç¡®ä¿ sam/sam_vit_h_4b8939.pth æ–‡ä»¶å­˜åœ¨")
        sam_checkpoint = None
    else:
        print(f"âœ“ ä½¿ç”¨æœ¬åœ° SAM æƒé‡: {sam_checkpoint}")
    
    print(f"Loading LISA model from {version}...")
    print(f"ä½¿ç”¨ç¼“å­˜ç›®å½•: {CACHE_DIR}")
    
    # åˆ›å»ºtokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        version,
        cache_dir=CACHE_DIR,
        model_max_length=model_max_length,
        padding_side="right",
        use_fast=False,
    )
    tokenizer.pad_token = tokenizer.unk_token
    seg_token_idx = tokenizer("[SEG]", add_special_tokens=False).input_ids[0]
    
    # è®¾ç½®ç²¾åº¦
    torch_dtype = torch.float32
    if precision == "bf16":
        torch_dtype = torch.bfloat16
    elif precision == "fp16":
        torch_dtype = torch.half
    
    # åŠ è½½æ¨¡å‹
    model_kwargs = {
        "torch_dtype": torch_dtype,
        "cache_dir": CACHE_DIR,
        "vision_pretrained": sam_checkpoint  # ä¼ é€’æœ¬åœ° SAM æƒé‡è·¯å¾„
    }
    model = LISAForCausalLM.from_pretrained(
        version, 
        low_cpu_mem_usage=True, 
        vision_tower=vision_tower, 
        seg_token_idx=seg_token_idx, 
        **model_kwargs
    )
    
    model.config.eos_token_id = tokenizer.eos_token_id
    model.config.bos_token_id = tokenizer.bos_token_id
    model.config.pad_token_id = tokenizer.pad_token_id
    
    # åˆå§‹åŒ–visionæ¨¡å—ï¼ˆä¼šä½¿ç”¨ vision_pretrained å‚æ•°åŠ è½½æœ¬åœ° SAM æƒé‡ï¼‰
    model.get_model().initialize_vision_modules(model.get_model().config)
    vision_tower = model.get_model().get_vision_tower()
    vision_tower.to(dtype=torch_dtype)
    
    # å°†æ¨¡å‹ç§»åˆ°GPU
    if precision == "bf16":
        model = model.bfloat16().cuda()
    elif precision == "fp16":
        model = model.half().cuda()
    else:
        model = model.float().cuda()
    
    vision_tower = model.get_model().get_vision_tower()
    vision_tower.to(device=local_rank)
    
    model.eval()
    
    # åˆ›å»ºå›¾åƒå¤„ç†å™¨
    clip_image_processor = CLIPImageProcessor.from_pretrained(
        model.config.vision_tower,
        cache_dir=CACHE_DIR
    )
    transform = ResizeLongestSide(image_size)
    
    # ä¿å­˜åˆ°å…¨å±€å˜é‡
    GLOBAL_MODEL = model
    GLOBAL_TOKENIZER = tokenizer
    GLOBAL_CLIP_PROCESSOR = clip_image_processor
    GLOBAL_TRANSFORM = transform
    
    # ä¿å­˜å‚æ•°
    class Args:
        pass
    args = Args()
    args.precision = precision
    args.use_mm_start_end = use_mm_start_end
    args.conv_type = conv_type
    args.seg_token_idx = seg_token_idx
    args.image_size = image_size
    GLOBAL_ARGS = args
    
    print("Model loaded successfully!")
    return model


def forward_single_sample(model, example):
    """
    LISAæ¨¡å‹å•æ ·æœ¬æ¨ç†
    
    å¯¹äºReasonSegä»»åŠ¡ï¼Œè¾“å‡ºåˆ†å‰²æ©ç 
    å¯¹äºRefCOCOä»»åŠ¡ï¼Œè¾“å‡ºæ–‡æœ¬å›ç­”
    """
    global GLOBAL_TOKENIZER, GLOBAL_CLIP_PROCESSOR, GLOBAL_TRANSFORM, GLOBAL_ARGS
    
    # è·å–è¾“å…¥
    image_pil = example['image']  # PIL Image
    text = example['text']  # str
    
    # å°†PIL Imageè½¬æ¢ä¸ºnumpy
    image_np = np.array(image_pil)
    if image_np.ndim == 2:  # ç°åº¦å›¾
        image_np = cv2.cvtColor(image_np, cv2.COLOR_GRAY2RGB)
    
    original_size_list = [image_np.shape[:2]]
    
    # å‡†å¤‡prompt
    prompt = DEFAULT_IMAGE_TOKEN + "\n" + text
    if GLOBAL_ARGS.use_mm_start_end:
        replace_token = (
            DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN
        )
        prompt = prompt.replace(DEFAULT_IMAGE_TOKEN, replace_token)
    
    conv = conversation_lib.conv_templates[GLOBAL_ARGS.conv_type].copy()
    conv.messages = []
    conv.append_message(conv.roles[0], prompt)
    conv.append_message(conv.roles[1], "")
    prompt = conv.get_prompt()
    
    # å¤„ç†å›¾åƒ - CLIP
    image_clip = (
        GLOBAL_CLIP_PROCESSOR.preprocess(image_np, return_tensors="pt")[
            "pixel_values"
        ][0]
        .unsqueeze(0)
        .cuda()
    )
    if GLOBAL_ARGS.precision == "bf16":
        image_clip = image_clip.bfloat16()
    elif GLOBAL_ARGS.precision == "fp16":
        image_clip = image_clip.half()
    else:
        image_clip = image_clip.float()
    
    # å¤„ç†å›¾åƒ - SAM
    image_sam = GLOBAL_TRANSFORM.apply_image(image_np)
    resize_list = [image_sam.shape[:2]]
    
    image_sam = (
        preprocess(torch.from_numpy(image_sam).permute(2, 0, 1).contiguous())
        .unsqueeze(0)
        .cuda()
    )
    if GLOBAL_ARGS.precision == "bf16":
        image_sam = image_sam.bfloat16()
    elif GLOBAL_ARGS.precision == "fp16":
        image_sam = image_sam.half()
    else:
        image_sam = image_sam.float()
    
    # Tokenize
    input_ids = tokenizer_image_token(prompt, GLOBAL_TOKENIZER, return_tensors="pt")
    input_ids = input_ids.unsqueeze(0).cuda()
    
    # æ¨¡å‹æ¨ç†
    with torch.no_grad():
        output_ids, pred_masks = model.evaluate(
            image_clip,
            image_sam,
            input_ids,
            resize_list,
            original_size_list,
            max_new_tokens=512,
            tokenizer=GLOBAL_TOKENIZER,
        )
    
    # è§£ç æ–‡æœ¬è¾“å‡º
    output_ids = output_ids[0][output_ids[0] != IMAGE_TOKEN_INDEX]
    text_output = GLOBAL_TOKENIZER.decode(output_ids, skip_special_tokens=False)
    text_output = text_output.replace("\n", "").replace("  ", " ")
    
    # å¤„ç†æ©ç è¾“å‡º
    pred_mask = None
    if pred_masks and len(pred_masks) > 0:
        for mask in pred_masks:
            if mask.shape[0] > 0:
                pred_mask = mask.detach().cpu().numpy()[0]
                pred_mask = (pred_mask > 0).astype(np.uint8)
                break
    
    # æ ¹æ®ä»»åŠ¡ç±»å‹è¿”å›ç»“æœ
    result = {
        'answer': text_output,  # RefCOCOéœ€è¦
    }
    
    if pred_mask is not None:
        result['mask'] = pred_mask  # ReasonSegéœ€è¦
    
    return result


def forward_batch_samples(model, examples):
    """
    æ‰¹é‡æ¨ç† - LISAä¸å¤ªæ”¯æŒæ‰¹é‡ï¼Œæ‰€ä»¥é€ä¸ªå¤„ç†
    """
    predictions = []
    for example in examples:
        pred = forward_single_sample(model, example)
        predictions.append(pred)
    return predictions


def main():
    """
    ä¸»å‡½æ•° - æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨
    """
    print("="*50)
    print("LISA Model Evaluation Example")
    print("="*50)
    print(f"\nğŸ’¡ æ¨¡å‹é…ç½®ä¿¡æ¯:")
    print(f"   - æ¨¡å‹ç¼“å­˜ç›®å½•: {CACHE_DIR}")
    print(f"   - SAM æƒé‡è·¯å¾„: {SAM_CHECKPOINT_PATH}")
    if os.path.exists(SAM_CHECKPOINT_PATH):
        print(f"   âœ“ SAM æƒé‡æ–‡ä»¶å·²æ‰¾åˆ°")
    else:
        print(f"   âœ— SAM æƒé‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„")
    print()
    
    # ============= ç¤ºä¾‹1: ReasonSegæ•°æ®é›†è¯„ä¼° =============
    print("\n[Example 1] ReasonSeg Dataset Evaluation")
    print("-"*50)
    
    # 1. åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœä½ æœ‰æœ¬åœ°æƒé‡ï¼Œä¿®æ”¹versionå‚æ•°ï¼‰
    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ä½ å·²ç»ä¸‹è½½äº†LISAæ¨¡å‹æƒé‡
    # å¦‚æœæ²¡æœ‰ï¼Œè¯·å…ˆä¸‹è½½æˆ–æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹è·¯å¾„
    try:
        model = load_model(
            version="xinlai/LISA-13B-llama2-v1",  # æˆ–æœ¬åœ°è·¯å¾„
            precision="fp32",  # æ ¹æ®æ˜¾å­˜é€‰æ‹©: fp32, fp16, bf16
            vision_tower="openai/clip-vit-large-patch14",
        )
    except Exception as e:
        print(f"Error loading model: {e}")
        print("æç¤º: è¯·ç¡®ä¿å·²å®‰è£…LISAå¹¶ä¸‹è½½äº†æ¨¡å‹æƒé‡")
        print("æˆ–è€…ä¿®æ”¹versionå‚æ•°æŒ‡å‘æœ¬åœ°æ¨¡å‹è·¯å¾„")
        return
    
    # 2. åŠ è½½ReasonSegæ•°æ®é›†
    # å‡è®¾ä½ çš„ReasonSegæ•°æ®åœ¨ä¸Šçº§ç›®å½•çš„ReasonSeg/valæ–‡ä»¶å¤¹
    reasonseg_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "ReasonSeg", "val")
    
    if os.path.exists(reasonseg_path):
        print(f"Loading ReasonSeg dataset from: {reasonseg_path}")
        dataset = ReasonSegDataset(image_folder=reasonseg_path)
        print(f"Dataset size: {len(dataset)}")
        
        # 3. è¿è¡Œé¢„æµ‹ï¼ˆè¿™é‡Œåªæµ‹è¯•å‰5ä¸ªæ ·æœ¬ä½œä¸ºæ¼”ç¤ºï¼‰
        print("\nRunning predictions on first 5 samples...")
        
        # åˆ›å»ºå°çš„æµ‹è¯•é›†
        class SmallDataset:
            def __init__(self, dataset, num_samples=5):
                self.dataset = dataset
                self.num_samples = min(num_samples, len(dataset))
            
            def __len__(self):
                return self.num_samples
            
            def __getitem__(self, idx):
                return self.dataset[idx]
        
        small_dataset = SmallDataset(dataset, num_samples=5)
        
        # è¿è¡Œé¢„æµ‹
        results = predict_model_by_single_sample(model, small_dataset)
        
        # 4. è®¡ç®—æŒ‡æ ‡
        print("\nComputing metrics...")
        metrics = compute_metrics(dataset, results)
        
        print("\n" + "="*50)
        print("ReasonSeg Evaluation Results:")
        print("="*50)
        for key, value in metrics.items():
            print(f"{key}: {value:.4f}")
    else:
        print(f"ReasonSeg dataset not found at: {reasonseg_path}")
        print("è¯·ç¡®ä¿æ•°æ®é›†è·¯å¾„æ­£ç¡®")
    
    
    # ============= ç¤ºä¾‹2: RefCOCOæ•°æ®é›†è¯„ä¼° =============
    print("\n\n[Example 2] RefCOCO Dataset Evaluation")
    print("-"*50)
    
    # å‡è®¾ä½ æœ‰RefCOCOçš„JSONLæ–‡ä»¶
    refcoco_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data", "refcoco", "refcoco_val.jsonl")
    
    if os.path.exists(refcoco_path):
        print(f"Loading RefCOCO dataset from: {refcoco_path}")
        
        # å¯¹äºRefCOCOï¼Œä¿®æ”¹promptæ¨¡æ¿
        refcoco_dataset = RefCOCODataset(
            jsonl_path=refcoco_path,
            prompt_template="Can you segment the {} in this image?"
        )
        print(f"Dataset size: {len(refcoco_dataset)}")
        
        # è¿è¡Œé¢„æµ‹ï¼ˆæµ‹è¯•å‰5ä¸ªï¼‰
        small_refcoco = SmallDataset(refcoco_dataset, num_samples=5)
        results = predict_model_by_single_sample(model, small_refcoco)
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = compute_metrics(refcoco_dataset, results)
        
        print("\n" + "="*50)
        print("RefCOCO Evaluation Results:")
        print("="*50)
        for key, value in metrics.items():
            print(f"{key}: {value:.4f}")
    else:
        print(f"RefCOCO dataset not found at: {refcoco_path}")
        print("å¦‚æœéœ€è¦è¯„ä¼°RefCOCOï¼Œè¯·å‡†å¤‡æ•°æ®é›†")
    
    print("\n" + "="*50)
    print("Evaluation completed!")
    print("="*50)


if __name__ == "__main__":
    main()

